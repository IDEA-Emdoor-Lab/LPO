dsw
192.168.72.23
60322
--nproc_per_node 8 --nnodes 2 --rdzv_id 30942 --rdzv_backend c10d --rdzv_endpoint 192.168.72.23:60322
15:4: not a valid test operator:  
15:4: not a valid test operator: 12.5
21:4: not a valid test operator: (
21:4: not a valid test operator: 535.129.03
15:4: not a valid test operator:  
15:4: not a valid test operator: 12.5
21:4: not a valid test operator: (
21:4: not a valid test operator: 535.129.03
W0714 14:08:49.890000 22734251775808 torch/distributed/run.py:778] 
W0714 14:08:49.890000 22734251775808 torch/distributed/run.py:778] *****************************************
W0714 14:08:49.890000 22734251775808 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0714 14:08:49.890000 22734251775808 torch/distributed/run.py:778] *****************************************
W0714 14:08:51.915000 22916697941824 torch/distributed/run.py:778] 
W0714 14:08:51.915000 22916697941824 torch/distributed/run.py:778] *****************************************
W0714 14:08:51.915000 22916697941824 torch/distributed/run.py:778] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0714 14:08:51.915000 22916697941824 torch/distributed/run.py:778] *****************************************
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:279: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:295: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:390: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  def forward(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/tensor_parallel/layers.py:429: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:88: UserWarning: To use flash-attn v3, please use the following commands to install: 
(1) pip install "git+https://github.com/Dao-AILab/flash-attention.git#egg=flashattn-hopper&subdirectory=hopper" 
(2) python_path=`python -c "import site; print(site.getsitepackages()[0])"` 
(3) mkdir -p $python_path/flashattn_hopper 
(4) wget -P $python_path/flashattn_hopper https://raw.githubusercontent.com/Dao-AILab/flash-attention/main/hopper/flash_attn_interface.py
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:84: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  scaled_e4m3_abstract = torch.library.impl_abstract("trt::quantize_fp8")(
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron_patch/model/llava/clip_encoder.py:26: UserWarning: The cvcuda environment does not exist. Install cvcuda and use it
  warnings.warn("The cvcuda environment does not exist. Install cvcuda and use it")
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000XPO double train_iters: 5000 --> 10000

XPO double train_iters: 5000 --> 10000
XPO double train_iters: 5000 --> 10000
using world size: 16, data-parallel size: 2, context-parallel size: 1 tensor-model-parallel size: 2, pipeline-model-parallel size: 4 
WARNING: Setting args.overlap_p2p_comm to False since non-interleaved schedule does not support overlapping p2p communication
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
------------------------ arguments ------------------------
XPO double train_iters: 5000 --> 10000
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.95
  adam_eps ........................................ 1e-08
  adaptive_seq_len ................................ False
XPO double train_iters: 5000 --> 10000
  add_bias_attn_fc ................................ True
  add_bias_linear ................................. False
  add_bias_linear_fc .............................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... True
  add_sft_loss .................................... False
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  allow_missing_vision_projection_checkpoint ...... False
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... True
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... False
  attention_dropout ............................... 0.0
  attention_head_type ............................. None
  attention_softmax_in_fp32 ....................... False
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  beta ............................................ 0.2
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ False
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  convert_checkpoint_from_megatron_to_transformers  False
  cpu_offloading .................................. False
  cpu_offloading_num_layers ....................... 0
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  cvcuda_image_processing ......................... False
  data_cache_path ................................. None
  data_dir ........................................ None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 2
  data_path ....................................... ['/cognitive_comp/songchao/ActorAgent/sft_datasets/audio/audio_train_20250703/train/xpo/xpo_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_save ................................. None
  dataloader_seq_length ........................... None
  dataloader_type ................................. single
  dataset ......................................... LLama-Pretrain-Idxmap
  dataset_config .................................. None
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  defer_embedding_wgrad_compute ................... False
  delay_grad_reduce ............................... True
  delay_param_gather .............................. False
  deprecated_use_mcore_models ..................... False
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  disable_vision_class_token ...................... False
  dist_ckpt_format ................................ torch_dist
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 3600
  embed_layernorm ................................. False
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_one_logger ............................... True
  enable_parallel_output .......................... True
  enable_shared_expert ............................ False
  encoder_num_layers .............................. 28
  encoder_pipeline_model_parallel_size ............ None
  encoder_seq_length .............................. 4096
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... True
  epochs .......................................... None
  eval_dev ........................................ False
  eval_fp32 ....................................... False
  eval_interval ................................... 10000
  eval_iters ...................................... 10
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  expert_interval ................................. 2
  expert_model_parallel_size ...................... 1
  expert_tensor_parallelism ....................... False
  extra_vocab_size ................................ 421
  ffn_hidden_size ................................. 18944
  finetune ........................................ False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  freeze_clip_vision_tower ........................ False
  freeze_llm ...................................... False
  freeze_LM ....................................... False
  freeze_ViT ...................................... False
  gamma ........................................... 10.0
  generation_length ............................... None
  global_batch_size ............................... 24
  glu_activation .................................. None
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... True
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.0
  hidden_size ..................................... 3584
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  image_aspect_ratio .............................. square
  image_folder .................................... 
  image_size ...................................... None
  image_tag_type .................................. 
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.008
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  input_len ....................................... 1
  intermediate_size ............................... None
  is_average ...................................... False
  iter_per_epoch .................................. 1250
  keep_last ....................................... False
  kv_channels ..................................... 128
  kv_lora_rank .................................... None
  label_smoothing ................................. 0.0
  language_model_type ............................. None
  lazy_mpu_init ................................... None
  load ............................................ /cognitive_comp/ccnl_common_data/wangrui/alm_sft_training/20250606/ref_ckpt/iter_000500_tp2_pp4
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... True
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... True
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  loss_type ....................................... sigmoid
  lr .............................................. 2e-07
  lr_decay_iters .................................. 9934
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 66
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. exponential
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_num_tiles ................................... 1
  max_padding_length .............................. 4096
  max_position_embeddings ......................... 131072
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... None
  micro_batch_size ................................ 6
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-07
  mm_projector_type ............................... None
  mm_use_im_patch_token ........................... False
  mm_use_im_start_end ............................. False
  mm_vision_select_layer .......................... None
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe ............................................. False
  moe_aux_loss_coeff .............................. 0.0
  moe_eval_capacity_factor ........................ 1.0
  moe_expert_capacity_factor ...................... None
  moe_expert_parallel_size ........................ None
  moe_extended_tp ................................. False
  moe_ffn_hidden_size ............................. None
  moe_grouped_gemm ................................ False
  moe_input_feature_slicing ....................... False
  moe_input_jitter_eps ............................ None
  moe_layer_freq .................................. 1
  moe_layer_recompute ............................. False
  moe_loss_coeff .................................. 0.01
  moe_min_capacity ................................ 4
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_topk ................................. 2
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_topk ........................................ 1
  moe_train_capacity_factor ....................... 1.0
  moe_z_loss_coeff ................................ None
  n_head_kv ....................................... None
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-06
  normalization ................................... RMSNorm
  num_attention_heads ............................. 28
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_experts ..................................... None
  num_fewshot ..................................... None
  num_frames ...................................... 1
  num_layers ...................................... 28
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 4
  num_shared_experts .............................. None
  num_workers ..................................... 4
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  online_evaluation_config ........................ None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  optimizer_offload_auto_threshold ................ 2147483648
  optimizer_offload_chunk_size .................... 33554432
  optimizer_offload_fraction ...................... 0.5
  optimizer_offload_policy ........................ static
  out_seq_length .................................. 1024
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. True
  overlap_p2p_comm ................................ False
  overlap_param_gather ............................ True
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  patch_size ...................................... 14
  patch_tokenizer_type ............................ Qwen2Tokenizer
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 4
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... rope
  position_encoding_2d ............................ False
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  prompt_path ..................................... None
  q_lora_rank ..................................... None
  qk_layernorm .................................... False
  qk_nope_head_dim ................................ None
  qk_rope_head_dim ................................ None
  query_in_block_prob ............................. 0.1
  r1 .............................................. 1.0
  r2 .............................................. 0.2
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  repetition_penalty .............................. 1.1
  reset_attention_mask ............................ True
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rotary_base ..................................... 1000000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_scale_factor ............................. 1
  rotary_scaling_factor ........................... 1
  rotary_seq_len_interpolation_factor ............. 1
  router_type ..................................... topk
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ /cognitive_comp/ccnl_common_data/large/sft_audio_data/output/text/20250607/train/checkpoints/checkpoint/xpo-mcore-qwen2.5-7B-lr-2e-7-minlr-1e-7-bs-6-gbs-24-seqlen-4096-pr-bf16-tp-2-pp-4-cp-1-ac-false-do-true-sp-true-ti-5000-wi-66
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 4096
  sequence_parallel ............................... True
  sgd_momentum .................................... 0.9
  shared_moe_ffn_hidden_size ...................... None
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  sliding_window .................................. None
  source_seq_len .................................. None
  spatial_merge_size .............................. 2
  spec ............................................ None
  special_tokens .................................. ['<image>']
  split ........................................... 99,1,0
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... True
  swin_backbone_type .............................. tiny
  target_seq_len .................................. None
  task_list ....................................... all
  temperature ..................................... 1.0
  temporal_patch_size ............................. 2
  tensor_model_parallel_size ...................... 2
  tensorboard_dir ................................. /cognitive_comp/ccnl_common_data/large/sft_audio_data/output/text/20250607/train/checkpoints/tensorboard/
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1
  test_data_path .................................. None
  test_mode ....................................... False
  text_generate_gt_file ........................... 
  text_generate_input_file ........................ 
  text_generate_output_file ....................... 
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  time ............................................ False
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_prompt_format ......................... None
  tokenizer_type .................................. NullTokenizer
  top_k ........................................... 0
  top_p ........................................... 0.0
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. True
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data ...................................... None
  train_data_path ................................. None
  train_iters ..................................... 10000
  train_mode ...................................... xpo
  train_samples ................................... None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 4
  transformer_timers .............................. False
  transformer_type ................................ megatron
  tune_mm_mlp_adapter ............................. False
  untie_embeddings_and_output_weights ............. True
  use_alibi_mask .................................. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... False
  use_distributed_optimizer ....................... True
  use_flash_attn .................................. False
  use_legacy_models ............................... False
  use_llama2_rotary_position_embeddings ........... False
  use_mistral_rotary_position_embeddings .......... False
  use_normhead .................................... False
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. True
  use_te .......................................... False
  use_thumbnail ................................... False
  use_tiling ...................................... False
  use_tp_pp_dp_mapping ............................ False
  use_tutel ....................................... False
  v_head_dim ...................................... None
  valid_data ...................................... None
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  verbosity ....................................... INFO
  version ......................................... plain
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_model_type ............................... clip
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vision_tower .................................... 
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... -1
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 16
  yaml_cfg ........................................ None
  z_loss_weight ................................... 0.0
-------------------- end of arguments ---------------------
> building NullTokenizer tokenizer ...
XPO double train_iters: 5000 --> 10000
 > padded vocab (size: 0) with 0 dummy tokens (new size: 0)
> initializing torch distributed ...
[W714 14:09:27.869081440 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.165970866 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.165970933 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.171877531 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.171932993 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.171995297 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.176085775 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.215201327 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.215222218 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.218258710 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.224902459 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.225063237 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
> setting tensorboard ...
[W714 14:09:27.229991906 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
[W714 14:09:27.235275190 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.377948327 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W714 14:09:27.362701780 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
> initialized tensor model parallel with size 2
> initialized pipeline model parallel with size 4
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/cognitive_comp/sunqianguo/workspace/tmp/wangrui/pai-megatron-patch/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.939 seconds
> compiling and loading fused kernels ...
>>> done with compiling and loading fused kernels. Compilation time: 2.034 seconds
[rank3]:W0714 14:09:31.179000 23386798102336 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank0]:W0714 14:09:31.179000 23068706146112 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank7]:W0714 14:09:31.179000 23270437287744 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank2]:W0714 14:09:31.179000 23246795302720 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank1]:W0714 14:09:31.179000 23270275401536 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank5]:W0714 14:09:31.179000 22363532490560 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank4]:W0714 14:09:31.182000 22783680632640 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank6]:W0714 14:09:31.182000 23362638071616 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682374430
[rank13]:W0714 14:09:31.222000 22781628786496 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank10]:W0714 14:09:31.222000 23333855786816 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank8]:W0714 14:09:31.222000 23374702008128 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank11]:W0714 14:09:31.222000 23201048020800 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank14]:W0714 14:09:31.222000 22477553100608 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank9]:W0714 14:09:31.222000 23349134604096 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank12]:W0714 14:09:31.222000 22834268411712 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank15]:W0714 14:09:31.223000 22536349439808 torch/distributed/distributed_c10d.py:2418] _object_to_tensor size: 28 hash value: 3540964629682352316
[rank12]:W0714 14:09:32.329000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.329000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.329000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.330000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.329000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.329000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.329000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.330000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.329000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.330000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.329000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.330000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.330000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.330000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.330000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.330000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.338000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.338000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.338000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.338000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.338000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.338000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.338000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.338000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.338000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.338000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.338000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.338000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.338000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.338000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.338000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.338000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.338000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.338000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.338000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.338000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.338000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.338000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.338000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.339000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.339000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.339000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.339000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.339000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.339000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.339000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.339000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.339000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.339000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.339000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.339000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.339000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.339000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.339000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.339000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.339000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.339000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.339000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.339000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.339000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.339000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.339000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.339000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.339000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.339000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.339000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.339000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.339000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.339000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.339000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.340000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.340000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.340000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.340000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.340000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.340000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank7]:W0714 14:09:32.340000 23270437287744 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.340000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.340000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.340000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.340000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.340000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.340000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.340000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.340000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.340000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.340000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.340000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.340000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.340000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.340000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.340000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.340000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.340000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.340000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.340000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.340000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.340000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.340000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.340000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.340000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.340000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.340000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.341000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.341000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.341000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.341000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.341000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.341000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.341000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.341000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.341000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.341000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.341000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.341000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.341000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.341000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank4]:W0714 14:09:32.341000 22783680632640 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank0]:W0714 14:09:32.341000 23068706146112 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank5]:W0714 14:09:32.341000 22363532490560 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.341000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.341000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.341000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.341000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.341000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank6]:W0714 14:09:32.341000 23362638071616 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.341000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.341000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank1]:W0714 14:09:32.342000 23270275401536 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.342000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.342000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank2]:W0714 14:09:32.342000 23246795302720 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.342000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.342000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.343000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank3]:W0714 14:09:32.343000 23386798102336 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.344000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.344000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.344000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.344000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.344000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.344000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.344000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.344000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.344000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.344000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.344000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.344000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.344000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.344000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.344000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.344000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.344000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.344000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.344000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.344000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.344000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.344000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.344000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.344000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.344000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.344000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.344000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.344000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.344000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.344000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.344000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.344000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.344000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.344000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.344000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.345000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.345000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.345000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.345000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.345000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.345000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.345000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.345000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.345000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.345000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.345000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.345000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.345000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.345000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.345000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.345000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.345000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.345000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.345000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.345000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.345000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.345000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.345000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.345000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.345000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.345000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank11]:W0714 14:09:32.345000 23201048020800 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.345000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.345000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.345000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank15]:W0714 14:09:32.345000 22536349439808 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.345000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.345000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.345000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.346000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.346000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.346000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.346000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.346000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.346000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.346000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.346000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank13]:W0714 14:09:32.346000 22781628786496 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.346000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.346000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank12]:W0714 14:09:32.346000 22834268411712 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.346000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.346000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.346000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.346000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.346000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.346000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.346000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.346000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.346000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.346000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.346000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank8]:W0714 14:09:32.346000 23374702008128 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank10]:W0714 14:09:32.346000 23333855786816 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.346000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.346000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.347000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.347000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.347000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.347000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.347000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank14]:W0714 14:09:32.347000 22477553100608 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.347000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.347000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.348000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
[rank9]:W0714 14:09:32.348000 23349134604096 torch/distributed/distributed_c10d.py:2428] _tensor_to_object size: 28 hash value: 10878699874869905965
!!! [UB] Global ranks on node 1: [8, 9, 10, 11, 12, 13, 14, 15]
!!! [UB] Number of physical nodes: 2
!!! [UB] Global ranks on node 0: [0, 1, 2, 3, 4, 5, 6, 7]
!!! [UB] Create Userbuffers Communicator
UB_TIMEOUT is set to 110 sec, 155100000000 cycles, freq: 1410000khz
MC NOT initialized and used
!!! [UBP2P] Register UBuf 1
!!! [UBP2P] Register UBuf 2
!!! [UBP2P] Register UBuf 3
!!! [UBP2P] Register UBuf 4
!!! [UB] Register UBuf 5
!!! [UB] Register UBuf 6
!!! [UB] Register UBuf 7
!!! [UB] Register UBuf 8
!!! [UB] Register UBuf 9
!!! [UB] Register UBuf 10
[rank6]:[W714 14:09:33.365405417 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank0]:[W714 14:09:33.365558401 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank3]:[W714 14:09:33.366873586 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank5]:[W714 14:09:33.368017535 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank1]:[W714 14:09:33.368363941 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank4]:[W714 14:09:33.368757524 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank7]:[W714 14:09:33.369521063 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank2]:[W714 14:09:33.370082416 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank11]:[W714 14:09:33.387346246 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank13]:[W714 14:09:33.388023398 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank10]:[W714 14:09:33.389063763 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank14]:[W714 14:09:33.389759286 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank9]:[W714 14:09:33.390378428 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank15]:[W714 14:09:33.390742835 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank8]:[W714 14:09:33.391168422 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
[rank12]:[W714 14:09:33.391516292 init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())
time to initialize megatron (seconds): 26.689
[after megatron is initialized] datetime: 2025-07-14 14:09:46 
> building Qwen2Tokenizer tokenizer ...
building qwen2 model ...
building qwen2 model in TE...
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:3256: UserWarning: window_size should be (-1, 0) or (>=0, 0) for attn_mask_type=causal
  warnings.warn(
 > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 815727360
 > number of parameters on (tensor, pipeline) model parallel rank (1, 1): 815727360
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 1088226048
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1088226048
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 815727360
 > number of parameters on (tensor, pipeline) model parallel rank (1, 3): 1088229632
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1088229632
 > number of parameters on (tensor, pipeline) model parallel rank (1, 2): 815727360
> learning rate decay style: cosine
 loading release checkpoint from /cognitive_comp/ccnl_common_data/wangrui/alm_sft_training/20250606/ref_ckpt/iter_000500_tp2_pp4
 checkpoint version 3.0
  successfully loaded checkpoint from /cognitive_comp/ccnl_common_data/wangrui/alm_sft_training/20250606/ref_ckpt/iter_000500_tp2_pp4 [ t 0, p 0 ] at iteration 0
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
  return func(*args, **kwargs)
(min, max) time across ranks (ms):
    load-checkpoint ................................: (2916.32, 2916.57)
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-07-14 14:09:53 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      240000
    validation: 480
    test:       240
> building train, validation, and test datasets for GPT ...
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd-hgx023: error: *** JOB 104288 ON hgx023 CANCELLED AT 2025-07-14T14:19:14 ***
slurmstepd-hgx023: error: *** STEP 104288.1 ON hgx023 CANCELLED AT 2025-07-14T14:19:14 ***
W0714 14:19:14.308000 22734251775808 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGTERM death signal, shutting down workers
W0714 14:19:14.309000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24249 closing signal SIGTERM
W0714 14:19:14.311000 22916697941824 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGTERM death signal, shutting down workers
W0714 14:19:14.312000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299546 closing signal SIGTERM
W0714 14:19:14.318000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299547 closing signal SIGTERM
W0714 14:19:14.318000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24250 closing signal SIGTERM
W0714 14:19:14.322000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24251 closing signal SIGTERM
W0714 14:19:14.322000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299548 closing signal SIGTERM
W0714 14:19:14.324000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24252 closing signal SIGTERM
W0714 14:19:14.331000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299549 closing signal SIGTERM
W0714 14:19:14.342000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299550 closing signal SIGTERM
W0714 14:19:14.347000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299551 closing signal SIGTERM
W0714 14:19:14.348000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24253 closing signal SIGTERM
W0714 14:19:14.353000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24254 closing signal SIGTERM
W0714 14:19:14.356000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24255 closing signal SIGTERM
W0714 14:19:14.359000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299552 closing signal SIGTERM
W0714 14:19:14.363000 22916697941824 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 3299553 closing signal SIGTERM
W0714 14:19:14.377000 22734251775808 torch/distributed/elastic/multiprocessing/api.py:857] Sending process 24256 closing signal SIGTERM
